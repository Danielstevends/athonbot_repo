{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6db228",
   "metadata": {},
   "source": [
    "# Tweet Intent Classification with Twitter Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a intent classification model using GRU model using twitter dataset. The dataset was scrapped using 'Twint'. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a46fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a087a",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30810524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/tweetlabels1000_labeled.xlsx - Sheet2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749bd9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1003, 4)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcorpus\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d51183",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with label = campaign, own tweet, & incomplete\n",
    "df = df.loc[~df['Label'].isin(['campaign','own tweet','incomplete'])].copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff031ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding & drop columns\n",
    "df['label'] = df['Label'].map({'indirect complaint': 0, 'remark': 1, 'negative remark': 2, 'direct compliment' : 3 , 'direct complaint' : 4, 'none' : 5, 'inquiry' : 6})\n",
    "df.drop(['No', 'Label', 'Username'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f649c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the tweet and label\n",
    "tweets, labels = list(df.Tweet), list(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d049ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893bb297",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "for data preprocessing we will do:\n",
    "- Lower the letter case\n",
    "- Cleansing (remove ascii, digits, punctuations, extra whitespaces, urls)\n",
    "- Normalization Indonesian words\n",
    "- Stemming 'kata berimbuhan'\n",
    "- Tokenization\n",
    "\n",
    "The tokenization process will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "<b>For padding sequence purpose, one way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6789c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lowercase\n",
    "df['tweet_lower'] = df['Tweet'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex Manipulation for text cleansing\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleansing(text):\n",
    "    \n",
    "       \n",
    "    # remove non ASCII (emoticon, chinese word, etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    \n",
    "    # remove digits (using regex) -> subtitute\n",
    "    text = re.sub('\\d+', '', text)\n",
    "    \n",
    "    # remove punctuation, reference: https://stackoverflow.com/a/34294398\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove whitespace in the beginning and end of sentence\n",
    "    text = text.strip()\n",
    "    \n",
    "    # remove extra whitespace in the middle of sentence (using regex)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # remove url in tweet (using regex)\n",
    "    text = re.sub(r\"\\bhttp\\w+\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f176eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_cleansing()\n",
    "df['tweet_clean'] = df['tweet_lower'].apply(lambda x: text_cleansing(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124982cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization indonesian words\n",
    "normalized_word = pd.read_csv('dataset/new_kamusalay.csv', header=None)\n",
    "data_dict = dict(zip(normalized_word[0], normalized_word[1]))\n",
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499aaa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    return ' '.join(data_dict.get(word, word) for word in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_normalized'] = df['tweet_clean'].apply(lambda x: normalize_text(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d75ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming 'kata berimbuhan'\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a06c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_stemmed'] = df['tweet_normalized'].apply(lambda x: stemmer.stem(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the tweet and label\n",
    "tweets, labels = list(df.tweet_stemmed), list(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff25462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b476352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", tweets[123])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(tweets)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[123])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f83a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc610f",
   "metadata": {},
   "source": [
    "\n",
    "## Bidirectional GRU Model with Random Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=7, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29710ea6",
   "metadata": {},
   "source": [
    "## Train and Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(n_splits=10, shuffle=False, random_state=None)\n",
    "\n",
    "# Separate the tweet and label\n",
    "tweets, labels = list(df.tweet_stemmed), list(df.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(tweets):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(tweets[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(tweets[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=30, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2680cad0",
   "metadata": {},
   "source": [
    "report = record\n",
    "report = report.to_excel('GRU.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4194075",
   "metadata": {},
   "source": [
    "## Predict new tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47ffeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessing pipeline\n",
    "def text_preprocessing(text):\n",
    "    # case folding\n",
    "    text = text.lower()\n",
    "    \n",
    "    # text cleansing\n",
    "    text = text_cleansing(text)\n",
    "    \n",
    "    # text normalization\n",
    "    text = normalize_text(text)\n",
    "    \n",
    "    # stemming\n",
    "    text = stemmer.stem(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d8531fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_cleansing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m new_tweet \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkok sekarang @indihome sinyalnya bapuk banget ya? hiksss :(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# preprocess\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m text_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mtext_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_tweet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m text_sequence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(text_cleaned)\n\u001b[0;32m      8\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mtext_preprocessing\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# text cleansing\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mtext_cleansing\u001b[49m(text)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# text normalization\u001b[39;00m\n\u001b[0;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m normalize_text(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_cleansing' is not defined"
     ]
    }
   ],
   "source": [
    "# unseen tweet\n",
    "new_tweet = \"kok sekarang @indihome sinyalnya bapuk banget ya? hiksss :(\"\n",
    "\n",
    "# preprocess\n",
    "text_cleaned = text_preprocessing(new_tweet)\n",
    "text_sequence = tokenizer.texts_to_sequences(text_cleaned)\n",
    "\n",
    "max_len = 100\n",
    "\n",
    "seq_padded = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# prediction\n",
    "pred_proba = model_0.predict(seq_padded)\n",
    "pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98978f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telkomathon3",
   "language": "python",
   "name": "telkomathon3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
